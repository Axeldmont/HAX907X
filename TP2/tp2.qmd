---
title: "HAX907X TP2 : Arbres"
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
---
```{python}
#| echo: false
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rc

from sklearn import tree, datasets
from tp_arbres_source import (rand_gauss, rand_bi_gauss, rand_tri_gauss,
                              rand_checkers, rand_clown,
                              plot_2d, frontiere)


rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})
params = {'axes.labelsize': 6,
          'font.size': 12,
          'legend.fontsize': 12,
          'text.usetex': False,
          'figure.figsize': (10, 12)}
plt.rcParams.update(params)

sns.set_context("poster")
sns.set_palette("colorblind")
sns.set_style("white")
_ = sns.axes_style()
```



```{python}
# Construction des classifieur
dt_entropy = tree.DecisionTreeClassifier(criterion='entropy')
dt_gini = tree.DecisionTreeClassifier(criterion='gini')

# Simulation de l'échantillon
n = 456
data = rand_checkers(n1=n//4, n2=n//4, n3=n//4, n4=n//4)
n_samples = len(data)
X = data[:,:2]
Y = np.asarray(data[:,-1], dtype=int)

# Entraînement des deux modèles
dt_gini.fit(X, Y)
dt_entropy.fit(X, Y)

print("Gini criterion")
print(dt_gini.score(X, Y))

print("Entropy criterion")
print(dt_entropy.score(X, Y))
```

```{python}
#| label: fig-first
#| echo: false
#| fig-cap: "Pourcentage d'erreurs commises en fonction de la profondeur maximale de l'arbre"

# Initialisation 
dmax = 12      # choix arbitraire   
scores_entropy = np.zeros(dmax)
scores_gini = np.zeros(dmax)

# Boucle principale
for i in range(dmax):
    # Critère : entropie
    dt_entropy = tree.DecisionTreeClassifier(criterion='entropy', 
                                             max_depth=i+1)
    dt_entropy.fit(X,Y)
    scores_entropy[i] = dt_entropy.score(X, Y)

    # Critère : indice de Gini
    dt_gini = tree.DecisionTreeClassifier(criterion='gini', 
                                          max_depth=i+1)
    dt_gini.fit(X,Y)
    scores_gini[i] = dt_gini.score(X,Y)

# Affichage des courbes d'erreur
plt.figure(figsize=(6,3.2))
plt.plot(1-scores_entropy, label="entropy")
plt.plot(1-scores_gini, label="gini")
plt.legend()
plt.xlabel('Profondeur maximale', fontsize=12)
plt.ylabel("Taux d'erreur", fontsize=12)
plt.draw()
```

```{python}
#| label: frontiere
#| fig-cap: "Frontières pour la meilleur profondeur (entropie)"

dt_entropy.max_depth = np.argmin(1-scores_entropy)+1
plt.figure(figsize=(6,3.2))
frontiere(lambda x: dt_entropy.predict(x.reshape((1, -1))), X, Y, step=100)
plt.draw()
print("Best scores with entropy criterion: ", dt_entropy.score(X, Y))
```